import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_news(query):
    url = f"https://news.google.com/search?q={query}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    articles = []
    for item in soup.select('article'):
        title = item.select_one('h3').text if item.select_one('h3') else 'No title'
        link = 'https://news.google.com' + item.select_one('a')['href'][1:] if item.select_one('a') else 'No link'
        pub_date = item.select_one('time')['datetime'] if item.select_one('time') else 'No date'
        summary = item.select_one('.xBbh9').text if item.select_one('.xBbh9') else 'No summary'
        
        articles.append({
            "언론사": "Google News",
            "뉴스 제목": title,
            "뉴스개시일": pub_date,
            "뉴스 링크": link,
            "Summary": summary
        })
    
    return articles

def save_to_excel(data, filename):
    df = pd.DataFrame(data)
    df.to_excel(filename, index=False)

if __name__ == "__main__":
    queries = ["IoT security", "electric cars security"]
    all_articles = []
    
    for query in queries:
        articles = get_news(query)
        all_articles.extend(articles)
    
    save_to_excel(all_articles, 'security_news.xlsx')
    df = pd.DataFrame(all_articles)
    df.to_csv('D:/repos/kisec/newsclip/security_news.csv', index=False)