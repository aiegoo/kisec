{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04778ea0-1c57-4924-9b8a-aee018ccacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5baa6815-b0cb-48ae-bae4-4eb1cd42358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for keyword: cybersecurity\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c88f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for keyword: google\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define keywords and the maximum number of items per day\n",
    "KEYWORDS = [\"google\"]\n",
    "MAX_ITEMS_PER_DAY = 5\n",
    "\n",
    "# Output file name\n",
    "OUTPUT_FILE = \"news_data.csv\"\n",
    "\n",
    "# Function to fetch news for a given keyword\n",
    "def fetch_news(keyword):\n",
    "    print(f\"Fetching news for keyword: {keyword}\")\n",
    "    url = f\"https://www.google.com/search?q={keyword}+news\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch news for {keyword}. HTTP Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extracting news items (customize selectors for your source)\n",
    "    results = []\n",
    "    for item in soup.select(\".BVG0Nb\")[:MAX_ITEMS_PER_DAY]:\n",
    "        title = item.text\n",
    "        link = item.find_parent(\"a\")[\"href\"]\n",
    "        summary = item.find_next_sibling(\".MUxGbd.wuQ4Ob.WZ8Tjf\").text if item.find_next_sibling(\".MUxGbd.wuQ4Ob.WZ8Tjf\") else \"No summary available\"\n",
    "        pub_date = datetime.now().strftime(\"%Y-%m-%d\")  # Simulated publication date\n",
    "        results.append({\n",
    "            \"언론사\": \"Google News\",\n",
    "            \"뉴스 제목\": title,\n",
    "            \"뉴스개시일\": pub_date,\n",
    "            \"뉴스 링크\": link,\n",
    "            \"Summary\": summary,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main crawling function\n",
    "def crawl_news():\n",
    "    all_news = []\n",
    "\n",
    "    for keyword in KEYWORDS:\n",
    "        news_items = fetch_news(keyword)\n",
    "        all_news.extend(news_items)\n",
    "\n",
    "    return all_news\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved {len(data)} items to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n",
    "# Run the crawler\n",
    "def main():\n",
    "    news_data = crawl_news()\n",
    "    save_to_csv(news_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
